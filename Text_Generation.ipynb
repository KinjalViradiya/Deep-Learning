{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Char-RNN\n",
    "Reading Ref: [http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)\n",
    "\n",
    "# Text Generation\n",
    "\n",
    "\n",
    "Download the Shakespear dataset from [Andrej's Blog](https://cs.stanford.edu/people/karpathy/char-rnn/shakespear.txt)\n",
    "\n",
    "Approach (broader view):\n",
    "- load the dataset\n",
    "- convert categorical values into some numerical representation; we'll create mapping of char to int\n",
    "- setup the sequence logic\n",
    "```\n",
    "if the sequence length is 4,\n",
    "HELL->O\n",
    "WORL->D\n",
    "```\n",
    "- set up the LSTM architecture\n",
    "- train it\n",
    "- see the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\z003w00f\\AppData\\Roaming\\Python\\Python36\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import necessary libraries\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset\n",
    "filename=\"shakespear.txt\"\n",
    "raw_text= open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text= raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create mapping of all unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', \"'\", ',', '-', '.', ':', ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total characters:  99993\n",
      "Total vocab:  36\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocabs = len(chars)\n",
    "print(\"Total characters: \", n_chars)\n",
    "print(\"Total vocab: \", n_vocabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total patterns:  99893\n"
     ]
    }
   ],
   "source": [
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length , 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append([char_to_int[seq_out]])\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "since LSTMs accept values in the form (no_of_sampels, time_steps, no_of_features), therefore<br>\n",
    "reshape dataX to this form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape dataX\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "\n",
    "# normalize\n",
    "X = X/float(n_vocabs)\n",
    "\n",
    "# one hot encoding using np_utils\n",
    "y = np_utils.to_categorical(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99893, 100, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99893, 36)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(rate=0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoints and callbacks\n",
    "filepath=\"SavedModels/weights-imporvement-{epoch: 02d}-{loss: .4f}-from-class.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating Text from pretrained/post training your LSTM\n",
    "\n",
    "filename = \"SavedModels/weights-imporvement- 50- 1.3274-from-class.hdf5\" # add the name of your best trained saved model's name here\n",
    "model.load_weights(filename)\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100\n",
      "99893/99893 [==============================] - 1137s 11ms/step - loss: 1.3208\n",
      "\n",
      "Epoch 00051: loss improved from inf to 1.32079, saving model to SavedModels/weights-imporvement- 51- 1.3208-from-class.hdf5\n",
      "Epoch 52/100\n",
      "99893/99893 [==============================] - 1133s 11ms/step - loss: 1.3189\n",
      "\n",
      "Epoch 00052: loss improved from 1.32079 to 1.31890, saving model to SavedModels/weights-imporvement- 52- 1.3189-from-class.hdf5\n",
      "Epoch 53/100\n",
      "99893/99893 [==============================] - 1129s 11ms/step - loss: 1.3109\n",
      "\n",
      "Epoch 00053: loss improved from 1.31890 to 1.31091, saving model to SavedModels/weights-imporvement- 53- 1.3109-from-class.hdf5\n",
      "Epoch 54/100\n",
      "99893/99893 [==============================] - 1128s 11ms/step - loss: 1.3098\n",
      "\n",
      "Epoch 00054: loss improved from 1.31091 to 1.30983, saving model to SavedModels/weights-imporvement- 54- 1.3098-from-class.hdf5\n",
      "Epoch 55/100\n",
      "99893/99893 [==============================] - 1131s 11ms/step - loss: 1.3061\n",
      "\n",
      "Epoch 00055: loss improved from 1.30983 to 1.30612, saving model to SavedModels/weights-imporvement- 55- 1.3061-from-class.hdf5\n",
      "Epoch 56/100\n",
      "99893/99893 [==============================] - 1132s 11ms/step - loss: 1.3024\n",
      "\n",
      "Epoch 00056: loss improved from 1.30612 to 1.30245, saving model to SavedModels/weights-imporvement- 56- 1.3024-from-class.hdf5\n",
      "Epoch 57/100\n",
      "99893/99893 [==============================] - 1132s 11ms/step - loss: 1.3021\n",
      "\n",
      "Epoch 00057: loss improved from 1.30245 to 1.30207, saving model to SavedModels/weights-imporvement- 57- 1.3021-from-class.hdf5\n",
      "Epoch 58/100\n",
      "99893/99893 [==============================] - 2536s 25ms/step - loss: 1.2927\n",
      "\n",
      "Epoch 00058: loss improved from 1.30207 to 1.29269, saving model to SavedModels/weights-imporvement- 58- 1.2927-from-class.hdf5\n",
      "Epoch 59/100\n",
      "99893/99893 [==============================] - 1147s 11ms/step - loss: 1.2915\n",
      "\n",
      "Epoch 00059: loss improved from 1.29269 to 1.29148, saving model to SavedModels/weights-imporvement- 59- 1.2915-from-class.hdf5\n",
      "Epoch 60/100\n",
      "99893/99893 [==============================] - 1144s 11ms/step - loss: 1.2865\n",
      "\n",
      "Epoch 00060: loss improved from 1.29148 to 1.28654, saving model to SavedModels/weights-imporvement- 60- 1.2865-from-class.hdf5\n",
      "Epoch 61/100\n",
      "99893/99893 [==============================] - 1138s 11ms/step - loss: 1.3281\n",
      "\n",
      "Epoch 00061: loss did not improve from 1.28654\n",
      "Epoch 62/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2759\n",
      "\n",
      "Epoch 00062: loss improved from 1.28654 to 1.27595, saving model to SavedModels/weights-imporvement- 62- 1.2759-from-class.hdf5\n",
      "Epoch 63/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2751\n",
      "\n",
      "Epoch 00063: loss improved from 1.27595 to 1.27509, saving model to SavedModels/weights-imporvement- 63- 1.2751-from-class.hdf5\n",
      "Epoch 64/100\n",
      "99893/99893 [==============================] - 1152s 12ms/step - loss: 1.2743\n",
      "\n",
      "Epoch 00064: loss improved from 1.27509 to 1.27428, saving model to SavedModels/weights-imporvement- 64- 1.2743-from-class.hdf5\n",
      "Epoch 65/100\n",
      "99893/99893 [==============================] - 1148s 11ms/step - loss: 1.2713\n",
      "\n",
      "Epoch 00065: loss improved from 1.27428 to 1.27131, saving model to SavedModels/weights-imporvement- 65- 1.2713-from-class.hdf5\n",
      "Epoch 66/100\n",
      "99893/99893 [==============================] - 1151s 12ms/step - loss: 1.2707\n",
      "\n",
      "Epoch 00066: loss improved from 1.27131 to 1.27073, saving model to SavedModels/weights-imporvement- 66- 1.2707-from-class.hdf5\n",
      "Epoch 67/100\n",
      "99893/99893 [==============================] - 1144s 11ms/step - loss: 1.2676\n",
      "\n",
      "Epoch 00067: loss improved from 1.27073 to 1.26763, saving model to SavedModels/weights-imporvement- 67- 1.2676-from-class.hdf5\n",
      "Epoch 68/100\n",
      "99893/99893 [==============================] - 1144s 11ms/step - loss: 1.2631\n",
      "\n",
      "Epoch 00068: loss improved from 1.26763 to 1.26311, saving model to SavedModels/weights-imporvement- 68- 1.2631-from-class.hdf5\n",
      "Epoch 69/100\n",
      "99893/99893 [==============================] - 1145s 11ms/step - loss: 1.2654\n",
      "\n",
      "Epoch 00069: loss did not improve from 1.26311\n",
      "Epoch 70/100\n",
      "99893/99893 [==============================] - 1145s 11ms/step - loss: 1.2535\n",
      "\n",
      "Epoch 00070: loss improved from 1.26311 to 1.25349, saving model to SavedModels/weights-imporvement- 70- 1.2535-from-class.hdf5\n",
      "Epoch 71/100\n",
      "99893/99893 [==============================] - 1156s 12ms/step - loss: 1.2558\n",
      "\n",
      "Epoch 00071: loss did not improve from 1.25349\n",
      "Epoch 72/100\n",
      "99893/99893 [==============================] - 1169s 12ms/step - loss: 1.2658\n",
      "\n",
      "Epoch 00072: loss did not improve from 1.25349\n",
      "Epoch 73/100\n",
      "99893/99893 [==============================] - 1162s 12ms/step - loss: 1.2522\n",
      "\n",
      "Epoch 00073: loss improved from 1.25349 to 1.25224, saving model to SavedModels/weights-imporvement- 73- 1.2522-from-class.hdf5\n",
      "Epoch 74/100\n",
      "99893/99893 [==============================] - 1157s 12ms/step - loss: 1.2486\n",
      "\n",
      "Epoch 00074: loss improved from 1.25224 to 1.24864, saving model to SavedModels/weights-imporvement- 74- 1.2486-from-class.hdf5\n",
      "Epoch 75/100\n",
      "99893/99893 [==============================] - 1156s 12ms/step - loss: 1.2487\n",
      "\n",
      "Epoch 00075: loss did not improve from 1.24864\n",
      "Epoch 76/100\n",
      "99893/99893 [==============================] - 1158s 12ms/step - loss: 1.2384\n",
      "\n",
      "Epoch 00076: loss improved from 1.24864 to 1.23838, saving model to SavedModels/weights-imporvement- 76- 1.2384-from-class.hdf5\n",
      "Epoch 77/100\n",
      "99893/99893 [==============================] - 1153s 12ms/step - loss: 1.2470\n",
      "\n",
      "Epoch 00077: loss did not improve from 1.23838\n",
      "Epoch 78/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2411\n",
      "\n",
      "Epoch 00078: loss did not improve from 1.23838\n",
      "Epoch 79/100\n",
      "99893/99893 [==============================] - 1145s 11ms/step - loss: 1.2499\n",
      "\n",
      "Epoch 00079: loss did not improve from 1.23838\n",
      "Epoch 80/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2532\n",
      "\n",
      "Epoch 00080: loss did not improve from 1.23838\n",
      "Epoch 81/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2410\n",
      "\n",
      "Epoch 00081: loss did not improve from 1.23838\n",
      "Epoch 82/100\n",
      "99893/99893 [==============================] - 1144s 11ms/step - loss: 1.2450\n",
      "\n",
      "Epoch 00082: loss did not improve from 1.23838\n",
      "Epoch 83/100\n",
      "99893/99893 [==============================] - 1145s 11ms/step - loss: 1.2351\n",
      "\n",
      "Epoch 00083: loss improved from 1.23838 to 1.23508, saving model to SavedModels/weights-imporvement- 83- 1.2351-from-class.hdf5\n",
      "Epoch 84/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2396\n",
      "\n",
      "Epoch 00084: loss did not improve from 1.23508\n",
      "Epoch 85/100\n",
      "99893/99893 [==============================] - 1147s 11ms/step - loss: 1.2352\n",
      "\n",
      "Epoch 00085: loss did not improve from 1.23508\n",
      "Epoch 86/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2418\n",
      "\n",
      "Epoch 00086: loss did not improve from 1.23508\n",
      "Epoch 87/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2321\n",
      "\n",
      "Epoch 00087: loss improved from 1.23508 to 1.23209, saving model to SavedModels/weights-imporvement- 87- 1.2321-from-class.hdf5\n",
      "Epoch 88/100\n",
      "99893/99893 [==============================] - 1145s 11ms/step - loss: 1.2279\n",
      "\n",
      "Epoch 00088: loss improved from 1.23209 to 1.22793, saving model to SavedModels/weights-imporvement- 88- 1.2279-from-class.hdf5\n",
      "Epoch 89/100\n",
      "99893/99893 [==============================] - 1146s 11ms/step - loss: 1.2340\n",
      "\n",
      "Epoch 00089: loss did not improve from 1.22793\n",
      "Epoch 90/100\n",
      "99893/99893 [==============================] - 1147s 11ms/step - loss: 1.2318\n",
      "\n",
      "Epoch 00090: loss did not improve from 1.22793\n",
      "Epoch 91/100\n",
      "99893/99893 [==============================] - 1145s 11ms/step - loss: 1.2227\n",
      "\n",
      "Epoch 00091: loss improved from 1.22793 to 1.22271, saving model to SavedModels/weights-imporvement- 91- 1.2227-from-class.hdf5\n",
      "Epoch 92/100\n",
      "99893/99893 [==============================] - 1145s 11ms/step - loss: 1.2181\n",
      "\n",
      "Epoch 00092: loss improved from 1.22271 to 1.21813, saving model to SavedModels/weights-imporvement- 92- 1.2181-from-class.hdf5\n",
      "Epoch 93/100\n",
      "99893/99893 [==============================] - 1158s 12ms/step - loss: 1.2178\n",
      "\n",
      "Epoch 00093: loss improved from 1.21813 to 1.21783, saving model to SavedModels/weights-imporvement- 93- 1.2178-from-class.hdf5\n",
      "Epoch 94/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99893/99893 [==============================] - 1154s 12ms/step - loss: 1.2199\n",
      "\n",
      "Epoch 00094: loss did not improve from 1.21783\n",
      "Epoch 95/100\n",
      "99893/99893 [==============================] - 1157s 12ms/step - loss: 1.2211\n",
      "\n",
      "Epoch 00095: loss did not improve from 1.21783\n",
      "Epoch 96/100\n",
      "99893/99893 [==============================] - 1147s 11ms/step - loss: 1.2196\n",
      "\n",
      "Epoch 00096: loss did not improve from 1.21783\n",
      "Epoch 97/100\n",
      "99893/99893 [==============================] - 1149s 11ms/step - loss: 1.2250\n",
      "\n",
      "Epoch 00097: loss did not improve from 1.21783\n",
      "Epoch 98/100\n",
      "99893/99893 [==============================] - 1149s 12ms/step - loss: 1.2281\n",
      "\n",
      "Epoch 00098: loss did not improve from 1.21783\n",
      "Epoch 99/100\n",
      "99893/99893 [==============================] - 1155s 12ms/step - loss: 1.2316\n",
      "\n",
      "Epoch 00099: loss did not improve from 1.21783\n",
      "Epoch 100/100\n",
      "99893/99893 [==============================] - 1161s 12ms/step - loss: 1.2255\n",
      "\n",
      "Epoch 00100: loss did not improve from 1.21783\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x270c8f4a4e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the LSTM\n",
    "\n",
    "model.fit(X, y, epochs=100, batch_size=64, callbacks=callbacks_list,initial_epoch=50) # do try out at diff epoch and batch sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT SEED:\n",
      "\" aff:\n",
      "what, say you thou art any happiness?\n",
      "\n",
      "first murderer:\n",
      "there's some but hot.\n",
      "\n",
      "orlando:\n",
      "basis i  \"\n",
      "\n",
      "lay see the fire that same that i will sea the will be sale;\n",
      "\n",
      "second lord:\n",
      "therefore, i lied ho suear like a song tiat the day that would poent be to my father's servers.\n",
      "\n",
      "antonio:\n",
      "a withug to the martiage,\n",
      "mord a song that i will prove be well and light to the puhen eriends; the heavens and to me,\n",
      "the way to be so.\n",
      "\n",
      "second servingman:\n",
      "now, he is may not prove her head and single shale as the pueen and to me,\n",
      "the which in the fortunes to the torenes;\n",
      "there are the heavens and conpent the sight of nur offecor.\n",
      "\n",
      "arutus:\n",
      "mo more than the will grom the but to cear as he was cesices him in thy food court,\n",
      "to see the duke of suranger, and in the pueen of the part.\n",
      "\n",
      "crutus:\n",
      "we will as my poenet, and in the pueen of the court,\n",
      "\n",
      "tecond lord:\n",
      "therefore, i lied ho saint and the surengt of lind of your frace and the single sraite,\n",
      "they have here iim to my father and in the foreht condemn'\n",
      "which now the cay that i will fo not were anl the simeles so cours:\n",
      "i will fo not be so.\n",
      "\n",
      "petruchio:\n",
      "nay, if t\n",
      "THE END.\n"
     ]
    }
   ],
   "source": [
    "# set up a random seed for starting\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "\n",
    "print(\"INPUT SEED:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[val] for val in pattern]), \"\\\"\")\n",
    "print()\n",
    "# generate characters from the generated output of LSTM\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x/float(n_vocabs)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1: len(pattern)]\n",
    "print(\"\\nTHE END.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
